<html>
<head>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']]
  }
};
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
  body {
    margin: 0;
    padding: 0;
    background-color: #ffffff;
    color: #222;
    font-family: -apple-system, BlinkMacSystemFont, "Helvetica Neue", Helvetica, Arial, sans-serif;
    line-height: 1.6;
  }

  a:link,a:visited {
    color: #0e7862;
    text-decoration: none;
  }
  a:hover {
    color: #24b597;
  }


  .content-margin-container {
    display: flex;
    width: 100%;
    justify-content: center;
    align-items: flex-start;
    box-sizing: border-box;
    margin-bottom: 0px;
  }

  .main-content-block {
    width: 60%;
    max-width: 900px;
    box-sizing: border-box;
    padding: 0 12px;
    background-color: #ffffff;
    margin-top: 0px;
    margin-bottom: 0px;
  }

  .margin-left-block {
    font-size: 14px;
    width: 180px;
    max-width: 180px;
    box-sizing: border-box;
    padding: 0 12px;
  }

  .margin-right-block {
    font-size: 13px;
    width: 220px;
    max-width: 220px;
    box-sizing: border-box;
    padding: 0 6px;
    color: #555;
  }


  #intro .margin-left-block {
    position: fixed;
    top: max(20%, 120px);
    left: 10px;
    max-width: 180px;
  }

  #prior_work .margin-left-block,
  #methods .margin-left-block,
  #discussion .margin-left-block,
  #conclusion .margin-left-block,
  #citations .margin-left-block {
    visibility: hidden;
  }

  .main-content-block.header-block {
    margin-top: 20px;
    margin-bottom: 0px;
  }

  table.header {
    font-weight: 400;
    font-size: 17px;
    width: 100%;
    border-collapse: collapse;
  }
  table td, table td * {
    vertical-align: middle;
    position: relative;
  }

  table.paper-code-tab {
    display: none;
  }

  h1 {
    font-size: 28px;
    margin-top: 20px;
    margin-bottom: 8px;
    font-weight: 600;
  }

  h3 {
    margin-top: 14px;
    margin-bottom: 6px;
  }

  img {
    max-width: 100%;
    height: auto;
    display: block;
    margin: 0 auto;
  }

  .my-video {
    max-width: 100%;
    height: auto;
    display: block;
    margin: 0 auto;
  }

  p {
    margin-top: 6px;
    margin-bottom: 6px;
  }

  .mathjax-mobile, .mathml-non-mobile { display: none; }
  .show-mathml .mathml-non-mobile { display: block; }
  .show-mathjax .mathjax-mobile { display: block; }

  .vid-mobile, .vid-non-mobile { display: none; }
  .show-vid-mobile .vid-mobile { display: block; }
  .show-vid-non-mobile .vid-non-mobile { display: block; }

  hr {
    height: 1px;
    border: none;
    background-color: #ddd;
    margin: 0px 0;
    max-width: 900px;
  }

  div.hypothesis {
    width: 80%;
    max-width: 800px;
    background-color: #EEE;
    border: 1px solid #ccc;
    border-radius: 6px;
    font-family: "SFMono-Regular", Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
    font-size: 16px;
    text-align: center;
    margin: 16px auto;
    padding: 12px;
  }

  div.citation {
    font-size: 0.9em;
    background-color:#fff;
    padding: 6px;
    border-top: 1px solid #eee;
  }

  .fade-in-inline {
    position: absolute;
    text-align: center;
    margin: auto;
    -webkit-mask-image: linear-gradient(to right,
                                      transparent 0%,
                                      transparent 40%,
                                      black 50%,
                                      black 90%,
                                      transparent 100%);
    mask-image: linear-gradient(to right,
                                transparent 0%,
                                transparent 40%,
                                black 50%,
                                black 90%,
                                transparent 100%);
    -webkit-mask-size: 8000% 100%;
    mask-size: 8000% 100%;
    animation-name: sweepMask;
    animation-duration: 4s;
    animation-iteration-count: infinite;
    animation-timing-function: linear;
    animation-delay: -1s;
  }

  .fade-in2-inline {
    animation-delay: 1s;
  }

  .inline-div {
    position: relative;
    display: inline-block;
    vertical-align: top;
    width: 50px;
  }

  @keyframes sweepMask {
    0%   { -webkit-mask-position: 0% 0; mask-position: 0% 0; }
    100% { -webkit-mask-position: 100% 0; mask-position: 100% 0; }
  }

  @media (max-width: 900px) {
    .content-margin-container {
      justify-content: flex-start;
    }
    .main-content-block {
      width: calc(100% - 220px);
    }
  }

  @media (max-width: 700px) {
    #intro .margin-left-block {
      position: static;
      top: auto;
      left: auto;
    }
    #prior_work .margin-left-block,
    #methods .margin-left-block,
    #discussion .margin-left-block,
    #conclusion .margin-left-block,
    #citations .margin-left-block {
      display: none;
    }
    .main-content-block {
      width: 100%;
      max-width: 100%;
      padding: 0 16px;
    }
    .margin-right-block {
      display: none;
    }
  }

  .project-header {
    display: flex;
    flex-direction: column;
    gap: 2px;
    padding-bottom: 4px;
  }

  .project-header .title-line {
    font-size: 24px;
    font-family: 'Courier New', Courier, monospace;
    margin-bottom: 0px;
  }

  .project-header .authors {
    font-size: 17px;
    display: flex;
    flex-direction: row;
    gap: 8px;
    align-items: center;
  }

  .project-header .subtitle {
    font-size: 18px;
    margin-top: 0px;
  }

  table {
    margin-left: auto;
    margin-right: auto;
  }

  table.results-table {
    border-collapse: collapse;
    margin: 10px auto;
    font-size: 14px;
  }

  table.results-table th,
  table.results-table td {
    border: 1px solid #ccc;
    padding: 6px 10px;
    text-align: center;
  }

  table.results-table th {
    background-color: #f5f5f5;
  }

  .table-caption {
    font-size: 0.9rem;
    color: #555;
    text-align: center;
    margin-bottom: 4px;
  }


</style>

<title>6.7960 Fall 2025 Project</title>
<meta property="og:title" content="6.7960 Fall 2025 Project" />
<meta charset="UTF-8">
</head>

<body>

  <div class="content-margin-container">
    <div class="margin-left-block">
    </div>
    <div class="main-content-block header-block">
      <div class="project-header">
		<div class="title-line">Switching Domains: mmWave 3D Reconstruction</div>
		<div class="authors">
			<a href="your_website">Christopher Liem</a>
			and
			<a href="your_partner's_website">Yibo Cheng</a>
		</div>

		<div class="subtitle">Final project for 6.7960, MIT</div>
	  </div>

      <hr>
    </div>
    <div class="margin-right-block">
      <!-- optional header margin note -->
    </div>
  </div>

  <div class="content-margin-container" id="intro">
    <div class="margin-left-block">
      <b style="font-size:16px">Outline</b><br><br>
      <a href="#intro">Introduction</a><br><br>
      <a href="#prior_work">Prior Work</a><br><br>
      <a href="#methods">Methods & Experiments</a><br><br>
      <a href="#discussion">Discussion</a><br><br>
      <a href="#conclusion">Conclusion</a><br><br>
    </div>
  </div>

  <div class="content-margin-container" id="intro-text">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
      <h1>Introduction</h1>
	  <hr>
      <p>
        A point cloud is a set of discrete points in 3D space that, when concatenated together, can represent the geometry of an object or scene.
        Point clouds play a central role in autonomous driving, medical imaging, robotic manipulation, and other applications. For example,
        autonomous vehicles typically utilize LiDAR sensors to generate point clouds for real-time 3D perception of their environment. These
        point clouds provide precise information about the distance and shape of surrounding objects that purely image-based techniques can't achieve.
        Combined with semantic understanding, this information allows the vehicle to recognize potentially dangerous situations and decide the best
        action to take.
      </p>
      <p>
        In practice, point clouds are often incomplete due to occlusions, limited sensor resolution, missing object parts,
        or noise in the measurement process. This motivates the problem of <em>point cloud completion</em>, where the goal is
        to reconstruct the full underlying 3D shape from a partial observation. Point cloud completion has become an active
        area of research, with methods ranging from classical geometric reconstruction to modern deep learning approaches.
      </p>
      <p>
        In parallel, there has been growing interest in millimeter waves, or mmWave for short. mmWaves are radio-frequency signals
        that offer massive bandwidths for communication, which companies such as Verizon exploit for their 5G network. More interestingly to us, mmWaves can travel
        through occlusions such as cardboard or fabric, allowing us to obtain point clouds of objects that are
        partially hidden or completely occluded in the visible spectrum. However, mmWave measurements exhibit very different
        physical characteristics from visible-light sensors: they are highly sparse, specular, and noisy, with limited angular
        resolution. As a result, deep models trained on vision-like partial point clouds fail to generalize directly to mmWave data.
      </p>

      <div class="hypothesis">
        <b>Central hypothesis.</b><br>
        We hypothesize that adapting point cloud completion models to mmWave sensing requires explicitly aligning the model’s
        inductive biases with mmWave physics. Concretely, we propose that
        <ul>
          <li>A physics-aware synthetic data pipeline that encodes mmWave reflection geometry,</li>
          <li>A full-reconstruction architecture that can denoise and reinterpret noisy partial inputs, and</li>
          <li>A density-aware loss that regularizes local point distributions</li>
        </ul>
        together will substantially improve reconstruction quality on real mmWave data compared to a state-of-the-art baseline
        designed for vision-style partials.
      </div>

      <p>
        In this project, we build on PoinTr, a transformer-based point cloud completion model, and adapt it to the mmWave domain.
        We introduce a synthetic dataset that incorporates mmWave reflection geometry, modify the architecture to predict the full
        point cloud (rather than only the missing part), and replace the standard Chamfer Distance loss with a density-aware variant.
        We then evaluate our model on both synthetic data and real-world mmWave measurements, analyze the learned latent
        representations, and discuss how our findings shed light on the role of inductive bias and latent manifolds in deep
        learning for 3D perception.
      </p>
    </div>
    <div class="margin-right-block">
    </div>
  </div>

  <div class="content-margin-container" id="prior_work">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
      <h1>Prior Work</h1>
	  <hr>
      <p>
        Research on completing 3D shapes from partial point clouds spans a progression of methods, beginning
        from geometric-based algorithms and evolving towards modern deep-learning approaches.
      </p>
      <p>
        A major field of geometric surface reconstruction methods are those that interpolate a point cloud using
        the existing geometry without any additional information. For example, Delaunay-based methods exploit
        the fact that the reconstructed triangulated surface can be formed by a subcomplex of the Delaunay triangulation, which
        connects a set of points with non-overlapping triangles such that no point is inside the circumscribed circle within any triangle ([<a href="#ref_1">1</a>], [<a href="#ref_2">2</a>]). While these
        methods have provable geometric and topological guarantees, they require dense sampling of the desired surface which is often
        impractical for imperfect real-world data. This limitation motivates a transition toward learning-based approaches, which aim
        to infer missing geometry based on learned priors rather than strict analytical rules.
      </p>
      <p>
        More recently, transformer-based models such as PoinTr leverage attention mechanisms to capture the geometric relationships
        between points [<a href="#ref_3">3</a>]. The pipeline for PoinTr can be summarized into two key points:
        <ol>
          <li>
            The point clouds in a local region are condensed into a feature vector which is called a "Point Proxy." Thus, the input point cloud is converted
            into a sequence of Point Proxies which are then fed into the rest of the pipeline for processing.
          </li>
          <li>
            To solve point cloud completion as a set-to-set translation problem, an encoder-decoder architecture is used. Given a set of Point Proxies,
            or partial point clouds, we wish to output a set of complete point clouds. To achieve this, the encoder maps the Point Proxies into
            a latent space that hopefully preserves important features which the decoder uses to infer the missing points. Specifically,
            geometry-aware transformer blocks are used to model the pairwise geometric relations between points which are crucial for the
            decoder to reason about the missing regions. This allows the model to leverage the inductive biases of 3D geometric structures
            in point clouds.
          </li>
        </ol>
      </p>
      <p>
        When benchmarked using the L1 Chamfer Distance on the PCN dataset, it was shown that PoinTr had substantially better performance
        than other state-of-the-art methods such as TopNet and Point Cloud Completion Network, both of which are also learning-based approaches
        to the same problem ([<a href="#ref_4">4</a>], [<a href="#ref_5">5</a>]). It is important to note, however, that all of these existing methods are able to produce reasonable outputs when given
        incomplete data.
      </p>
      <p>
        While prior models may perform well on vision-like partial point clouds, they don't generalize well to
        the sparse and noisy partials produced by mmWave measurements. Thus, we wish to build off of the recent
        advancements in learning-based 3D reconstruction to solve the domain-specific problem of mmWave-produced point cloud
        completion.
      </p>
    </div>
    <div class="margin-right-block"></div>
  </div>

  <div class="content-margin-container" id="methods">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
      <h1>Methods & Experiments</h1>
	  <hr>
    <p>
      We build upon PoinTr, a transformer-based point cloud completion model, and introduce several key modifications to adapt
      it to the unique physical characteristics of mmWave radar signals. Below, we describe our synthetic dataset generation pipeline, architectural changes, and density-aware loss.</p>


	  <h3>Synthetic Dataset for mmWave-Style Partials</h3>
	  <p>
		To train a reconstruction network tailored to mmWave data, we create a synthetic dataset designed to mimic mmWave reflections. Our dataset consists of three publicly available 3D shape repositories:
		<ul>
			<li><b>OmniObject3D</b>: A large multi-view 3D dataset comprised of roughly 6,000 scans of real objects in 190 categories.</li>
      <figure style="max-width: 400px; margin: 0 auto; text-align: center;">
        <img src="images/omniobject.png" alt="OmniObject3D Examples" style="max-width: 100%; height: auto;">
        <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
          Figure 1: Example objects from the OmniObject3D dataset
        </figcaption>
      </figure>
			<li><b>Toys4K-3D</b>: A dataset of 4,179 3D objects in 105 categories, focusing on toys and household objects commonly
          encountered by infants and children.</li>
      <figure style="max-width: 500px; margin: 0 auto; text-align: center;">
        <img src="images/toys4k.png" alt="Toys4K-3D Examples" style="max-width: 100%; height: auto;">
        <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
          Figure 2: Example objects from the Toys4K-3D dataset
        </figcaption>
      </figure>
			<li><b>Thingiverse (Objaverse)</b>: A subset of around 15k objects from the Thingiverse platform, mostly
          consisting of untextured meshes. These objects are useful for learning diverse shape priors. </li>
      <figure style="max-width: 500px; margin: 0 auto; text-align: center;">
        <img src="images/thingiverse.png" alt="Thingiverse Examples" style="max-width: 100%; height: auto;">
        <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
          Figure 3: Example meshes from the Thingiverse subset
        </figcaption>
      </figure>
		</ul>
	  </p>

    <p>
      Existing point cloud completion models are typically trained on vision-style partials from visible-light sensors such as RGB-D cameras or LiDAR. mmWave signals, however,
      produce sparse, specular, anisotropic reflections with significant noise and limited angular resolution. To close the domain gap, we encode mmWave physics into our synthetic training samples.
    </p>

    <p>
    We illustrate the preprocessing pipeline using a lion sample, assuming the radar is located above it.
    </p>

    <figure style="max-width: 300px; margin: 0 auto; text-align: center;">
      <img src="images/complete_lion.png" alt="Lion" style="max-width: 100%; height: auto;">
      <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
        Figure 4: Example complete point cloud of a lion mesh
      </figcaption>
    </figure>

    <p>
      To embed mmWave specularity directly into these synthetic samples, we retain only the points that would (1) produce a specular return and (2) lie on the surface facing the radar.
      The first condition models the fact that mmWave reflections follow a specular law: a point is visible only if the incoming ray and reflected ray form equal angles with the
      surface normal in their shared plane. The second condition ensures the training partials only include geometry that physically “faces” the radar, since
      mmWave sensors rarely receive energy from back-facing or oblique surfaces.
    </p>
    <p>
      Formally, we define the set of partial observations $\mathcal{P}_\text{obs} $ as
    </p>

    <p>
        $$
        \mathcal{P}_{\text{obs}} = \left\{ s_i \in \mathcal{P}_\text{full} \,\middle|\, \theta_R(s_i) < \gamma,\; V(s_i) = 1 \right\}
        $$
        where $\mathcal{P}_\text{full}$ is the full 3D point cloud, $s_i$ is a sample point in $\mathcal{P}_\text{full}$, and
        $$
        \theta_R(s_i) = \min_{r \in R} \left|\arccos(n_i \cdot u_{r,i})\right|,\quad
        u_{r,i} = \frac{r - s_i}{\lVert r - s_i \rVert_2}.
        $$
        Here $R$ is the set of possible radar positions, $n_i$ is the surface normal at $s_i$, $u_{r,i}$ is the unit vector pointing from $s_i$ to $r$, and $\gamma$ is a threshold on
        the angular mismatch. The visibility indicator $V(s_i)$ is 1 when $s_i$ lies on the radar-facing surface and 0 otherwise.
      </p>

    <figure style="max-width: 400px; margin: 0 auto; text-align: center;">
      <img src="images/mmwave_mask.png" alt="mmwave_mask" style="max-width: 100%; height: auto;">
      <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
        Figure 5: Points that satisfy the mmWave specular reflection condition
      </figcaption>
    </figure>

    <figure style="max-width: 400px; margin: 0 auto; text-align: center;">
      <img src="images/los_mask.png" alt="los_mask" style="max-width: 100%; height: auto;">
      <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
        Figure 6: Points that lie on the surface facing the radar (line-of-sight constraint)
      </figcaption>
    </figure>

    <figure style="max-width: 400px; margin: 0 auto; text-align: center;">
      <img src="images/after_mask.png" alt="after_mask" style="max-width: 100%; height: auto;">
      <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
        Figure 7: Resulting partial point cloud after applying both constraints ($\mathcal{P}_\text{obs}$)
      </figcaption>
    </figure>

    <p>
      Next, to simulate the limited resolution of mmWave radars, we remove the outlier points that are too isolated from their neighbors to produce $\mathcal{P}_\text{obs}'$. Specifically, we cluster points based on their Euclidean distance:
      if two points are within 0.03 meters, they are assigned to the same cluster. Clusters with fewer than 100 points are filtered out. This step mimics the fact
        that very small or isolated returns are often lost in real mmWave measurements.
    </p>

    <figure style="max-width: 400px; margin: 0 auto; text-align: center;">
      <img src="images/outlier_removal.png" alt="outlier removal" style="max-width: 100%; height: auto;">
      <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
        Figure 8: Partial point cloud after outlier removal and cluster filtering ($\mathcal{P}_\text{obs}'$)
      </figcaption>
    </figure>

	  <p>
		  Finally, we inject Gaussian noise into the synthetic partial point clouds $\mathcal{P}_\text{obs}'$ to produce $\mathcal{P}_\text{obs}''$, modeling the substantial noise present in
      real mmWave measurements.

    </p>

    <figure style="max-width: 500px; margin: 0 auto; text-align: center;">
      <img src="images/gaussian_noise.png" alt="add Gaussian Noise" style="max-width: 100%; height: auto;">
      <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
        Figure 9: Synthetic mmWave-style partial with added Gaussian noise ($\mathcal{P}_\text{obs}''$)
      </figcaption>
    </figure>


    <h3>Full Reconstruction</h3>
    <p>
      The original PoinTr architecture predicts only the missing part of the point cloud and then concatenates the predicted
      completion with the input partial to form the final output. This design assumes that the input partial is relatively
      clean and reliable. For mmWave data, however, the partial observations are often very noisy and contain spurious
      reflections. As a result, PoinTr model never has an opportunity to reinterpret and denoise the input partial.
    </p>

    <figure style="max-width: 400px; margin: 0 auto; text-align: center;">
      <img src="images/use_origianl_input.png" alt="use_origianl_input" style="max-width: 100%; height: auto;">
      <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
        Figure 10: Original PoinTr pipeline: the noisy partial is directly concatenated with the predicted completion
      </figcaption>
    </figure>

    <p>
      To address this limitation, we modify the architecture to predict the <em>entire</em> point cloud rather than just the
      missing portion. The encoder still maps the noisy mmWave partial to a latent representation, but the decoder now outputs
      a full reconstruction of the object. This change gives the model the flexibility to reinterpret and effectively denoise
    the input partial, rather than being constrained to preserve it in the final output.
    </p>

    <figure style="max-width: 400px; margin: 0 auto; text-align: center;">
      <img src="images/full_reconstruction.png" alt="full_reconstruction" style="max-width: 100%; height: auto;">
      <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
        Figure 11: Modified architecture: the model predicts the full point cloud
      </figcaption>
    </figure>

    <h3>Density-aware Cost Function</h3>

    <p>
      During training, we noticed that the predicted point cloud tends to exhibit uneven point density, with some regions overly dense and others too sparse.
      This is because the original Chamfer Distance (CD) loss used in PoinTr treats all points equally, regardless of their local spatial distribution. As a result, the model can “pile up” many points in one area
        without being sufficiently penalized, as long as every ground-truth point is close to some predicted point and vice versa. For reference,
      the Chamfer Distance is given by
		$$
			\text{CD}=\frac{1}{|\mathcal{P}_\text{pred}|}\sum_{p\in \mathcal{P}_\text{pred}}\min_{g\in \mathcal{P}_\text{full}}||p-g||+\frac{1}{|\mathcal{P}_\text{full}|}\sum_{g\in \mathcal{P}_\text{full}}\min_{p\in \mathcal{P}_\text{pred}}||g-p||
		$$
    where $\mathcal{P}_\text{pred}$ is the predicted full point cloud and $\mathcal{P}_\text{full}$ is the ground-truth full point cloud. If one area of the
      point cloud is too dense (or sparse) vs the ground truth, then CD may not penalize it sufficiently.

    To encourage more uniform point distributions, we apply a density-aware cost function that weights points based on their local density.
    $$
        d_{\mathrm{DCD}}(\mathcal{P}_\text{pred}, \mathcal{P}_\text{full}) =
      \frac{1}{2} \left(
      \frac{1}{|\mathcal{P}_\text{pred}|} \sum_{p \in \mathcal{P}_\text{pred}}
      \left( 1 - \frac{1}{n_{\hat{g}}} e^{-\alpha \|p - \hat{g}\|_2} \right)
      +
      \frac{1}{|\mathcal{P}_\text{full}|} \sum_{g \in \mathcal{P}_\text{full}}
      \left( 1 - \frac{1}{n_{\hat{p}}} e^{-\alpha \|g - \hat{p}\|_2} \right)
      \right)
    $$
    where $\hat{g} = \min_{g \in \mathcal{P}_\text{full}} \|p - g\|_2$ and $\hat{p} = \min_{p \in \mathcal{P}_\text{pred}} \|g - p\|_2$. $\alpha$ is a temperature scaler, $n_{g} = |\mathcal{P}_\text{pred}^g |$ and $n_{p} = |\mathcal{P}_\text{full}^p |$.
    Intuitively, if a point is in a dense region (large $n_{\hat{g}}$ or $n_{\hat{r}}$), its contribution to the loss is down-weighted, encouraging the model to spread points more evenly.

	  </p>


    </div>
    <div class="margin-right-block"></div>
  </div>

  <div class="content-margin-container" id=".">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
      <h3>Experiments</h3>
      <p>
        To evaluate the performance of our model, we use real-world mmWave measurements of 61 objects from the MITO dataset [<a href="#ref_6">6</a>]. The dataset include daily objects with
		    diverse properties, both material and geometric, such as wooden v.s. metal and flat v.s. curved. To further
		    test real-world application, both line-of-sight and fully-obstructed mmWave data of each object are used.
      </p>



    </div>
    <div class="margin-right-block"></div>
  </div>

  <div class="content-margin-container" id="discussion">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
      <h1>Discussion</h1>
	  <hr>
    <h3>Results</h3>
    <p>
        Following prior work on point cloud completion, we report:
      </p>
      <ul>
        <li>
          <b>Chamfer Distance (CD).</b> Lower values indicate better geometric reconstruction.
        </li>
        <li>
          <b>F-Score at a fixed threshold.</b> Higher values indicate better overlap between the predicted and ground-truth
          point sets.
        </li>
      </ul>

      <h3>Quantitative Results</h3>
      <p>
        Table 1 summarizes the performance of PoinTr and our model on both the synthetic mmWave-style dataset and the MITO
        mmWave dataset. Lower Chamfer Distance and higher F-Score correspond to better reconstructions.
      </p>

      <div class="table-caption">
        Table 1: Quantitative comparison between PoinTr and our model on synthetic and real-world mmWave data.
      </div>
      <table class="results-table">
        <tr>
          <th rowspan="2">Model</th>
          <th colspan="2">Synthetic Dataset</th>
          <th colspan="2">Real-World Dataset</th>
        </tr>
        <tr>
          <th>Chamfer Distance</th>
          <th>F-Score</th>
          <th>Chamfer Distance</th>
          <th>F-Score</th>
        </tr>
        <tr>
          <th>PoinTr</th>
          <td>0.071</td>
          <td>70%</td>
          <td>0.104</td>
          <td>62%</td>
        </tr>
        <tr>
          <th>Our Model</th>
          <td>0.034</td>
          <td>91%</td>
          <td>0.069</td>
          <td>75%</td>
        </tr>
      </table>

      <p>
      Our model consistently outperforms PoinTr on both synthetic and real-world datasets, roughly halving the Chamfer
        Distance on synthetic data and substantially improving F-Score in all settings. These results support our central
        hypothesis: explicitly encoding mmWave physics into the data, allowing the model to predict full reconstructions, and
        regularizing point densities leads to meaningful gains in reconstruction quality.
      </p>
      <figure style="max-width: 400px; margin: 0 auto; text-align: center;">
        <img src="images/visualization.png" alt="visualization" style="max-width: 100%; height: auto;">
        <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
          Figure 12: Qualitative comparison between PoinTr and our model on mmWave data
        </figcaption>
      </figure>

      <h3>Latent Space Analysis and Inductive Bias</h3>
      <p>
        To better understand <em>why</em> our model performs better, we analyze the latent spaces of both models. We extract
        the encoder outputs as the latent representations of the input partial point clouds and apply PCA to reduce the
        dimensionality to 2D for visualization.
      </p>

      <figure style="max-width: 400px; margin: 0 auto; text-align: center;">
        <img src="images/encoder_pca.png" alt="encoder_pcader_pca" style="max-width: 100%; height: auto;">
        <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
          Figure 13: PCA visualization of encoder outputs for PoinTr (left) and our model (right)
        </figcaption>
      </figure>

      <p>
        We also visualize the PCA embeddings of the decoder outputs, which correspond to the point proxies used to generate
        the final point cloud.
      </p>

      <figure style="max-width: 400px; margin: 0 auto; text-align: center;">
        <img src="images/decoder_pca.png" alt="decoder_pcader_pca" style="max-width: 100%; height: auto;">
        <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
          Figure 14: PCA visualization of decoder outputs for PoinTr (left) and our model (right)
        </figcaption>
      </figure>

      <p>
        In both encoder and decoder latent spaces, our model maps different objects into more clearly separated regions, with
        clusters that correlate with object categories or coarse shape families. In contrast, PoinTr’s latent space exhibits
        substantial overlap among objects, suggesting that it struggles to disentangle different shapes under mmWave-style
        sparsity and noise.
      </p>

      <p>
        This observation supports the view that architectural and loss-function choices act as inductive biases that reshape
        the model’s latent manifold. By forcing the model to reconstruct the full point cloud and match density distributions,
        we implicitly encourage a representation space in which objects with different global geometries occupy distinct, more
        structured regions. This, in turn, makes it easier for the decoder to infer missing geometry even when the input
        observation is severely degraded by mmWave artifacts.
      </p>
      <h3>Limitations</h3>
      <p>
        Despite our improved performance in comparison to PoinTr, our model still has limitations in accuracy, specifically with small objects that have irregular shape. For the case of small objects,
        we tested our model on a large clamp, a medium clamp, and a small clamp. The real mmWave point cloud of the clamp was substantially lower resolution than the
        one for the large clamp, which resulted in a worse input to be fed to our model. As a result, our model's performance deteriorates as our test object gets smaller. This can be visualized in the figure
        below.
      </p>

      <figure style="max-width: 400px; margin: 0 auto; text-align: center;">
        <img src="images/clamps.png" alt="clamps" style="max-width: 100%; height: auto;">
        <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
          Figure 15: Clamps of different sizes
        </figcaption>
      </figure>

      <p>
        We additionally observe poor performance on fruits, which we attribute to their high curvature—geometry that real mmWave sensors weren't able to capture accurately.
        This limitation suggests that our synthetic data generation did not sufficiently model the complex mmWave behavior exhibited by such surfaces. Since our pipeline utilizes
        a simplified approximation of mmWave physics, future work could improve performance by incorporating more accurate approximations into the data filters.

      </p>
      <figure style="max-width: 300px; margin: 0 auto; text-align: center;">
        <img src="images/orange.png" alt="orange" style="max-width: 100%; height: auto;">
        <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
          Figure 16: Point cloud of an orange that combines input (blue), prediciton (red), and ground truth (green)
        </figcaption>
      </figure>
      <p>
        Finally, our model performs poorly on objects composed of regions with differing reflectivity, causing real-world mmWave point clouds to fragment into separate clusters. For instance, a
        hammer’s metallic head and wooden handle produce two distinct returns, yielding disjoint point cloud segments.
        Our synthetic data pipeline does not currently model such material-dependent reflectivity, leading to degraded performance on these mixed-material objects compared to those with uniform surfaces.
      </p>
      <figure style="max-width: 800px; margin: 0 auto; text-align: center;">
        <img src="images/hammer.png" alt="hammer" style="max-width: 100%; height: auto;">
        <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
          Figure 17: Point cloud of a hammer
        </figcaption>
      </figure>

      <p>Nonetheless, we successfully adapted the PoinTr backbone to achieve substantially better performance on mmWave measurements. This result is significant, as it illustrates how transfer learning
        can enable state-of-the-art models to be repurposed effectively for domain-specific applications, such as mmWave sensing.
      </p>
    </div>
    <div class="margin-right-block"></div>
  </div>

  <div class="content-margin-container" id="conclusion">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
      <h1>Conclusion</h1>
	  <hr>
      <p>
        In this project, we attempted to adapt state-of-the-art point cloud completion techniques to the fundamentally different sensing regime of mmWave radar. Unlike vision-based sensors,
        mmWave systems generate highly sparse, noisy, and specular measurements, making direct application of existing models ineffective. To address this challenge, we introduced a physics-aware
        synthetic data pipeline, a full-reconstruction architecture that enables implicit denoising, and a density-aware loss that promotes more uniform point distributions.
      </p>
      <p>
        Through experimentation on both synthetic data and real-world measurements from the MITO dataset, our model consistently outperforms PoinTr across Chamfer Distance and F-Score metrics.
        Qualitative results further demonstrate improved geometric fidelity, especially in cases where mmWave sparsity is severe. Latent-space analysis suggests that our architectural changes lead to
        more structured and discriminative feature representations, enabling the model to better distinguish object categories and infer missing geometry.

        Beyond performance gains, this work contributes to the broader understanding of deep learning in several ways:
        <ul>
          <li>It highlights the importance of incorporating domain-specific inductive biases into data generation and model design, rather than relying solely on generic architectures.</li>

          <li>It demonstrates how deep learning models behave under severe domain shift: models trained on vision-like data fail to generalize to mmWave inputs, whereas models equipped with the appropriate inductive biases can successfully transfer across sensing modalities.</li>

          <li>It illustrates that architectural and loss-function choices directly reshape the latent manifold: by enforcing full reconstruction and density-aware optimization, the model learns latent clusters that better reflect object categories and geometric relationships.</li>

          <li>It reinforces the broader understanding that deep learning is not just about scaling data or model size. Rather, the practice of producing meaningful datasets that capture our domain-specific properties is underscored.  </li>
        </ul>


        While our approach significantly improves reconstruction quality in comparison to PoinTr, challenges remain, especially for small or irregular objects that produce weak mmWave returns. Future work may integrate richer
        generative priors, differentiable radar simulation, or multi-sensor fusion to further enhance robustness.

        Overall, our findings demonstrate how the effectiveness of deep learning models depends critically on the inductive biases we embed into their data, architecture, and objectives. By aligning these biases with the physics of mmWave sensing,
        we move toward reliable 3D perception beyond the domain of traditional vision sensors.
      </p>
    </div>
    <div class="margin-right-block"></div>
  </div>

  <div class="content-margin-container" id="citations">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
      <div class='citation' id="references" style="height:auto"><br>
        <span style="font-size:16px">References:</span><br><br>
        <a id="ref_1"></a>[1] <a href="https://www.cs.jhu.edu/~misha/Fall13b/Papers/Cazals06.pdf">Delaunay Triangulation Based Surface
Reconstruction: Ideas and Algorithms</a>, Cazals et al., 2004<br><br>
        <a id="ref_2"></a>[2] <a href="https://inria.hal.science/hal-01017700/document">State of the Art in Surface Reconstruction from Point
Clouds</a>, Berger et al., 2014<br><br>
        <a id="ref_3"></a>[3]
<a href="https://arxiv.org/abs/2108.08839">PoinTr: Diverse Point Cloud Completion with Geometry-Aware Transformers</a>,
Yu et al., 2021 <br><br>
        <a id = "ref_4"></a>[4] <a href="https://ieeexplore.ieee.org/document/8953650">TopNet: Structural Point Cloud Decoder</a>,
        Tchapmi et al., 2019 <br><br>
        <a id = "ref_5"></a>[5] <a href="https://arxiv.org/abs/1808.00671">PCN: Point Cloud Completion Network</a>, Yuan et al., 2018 <br><br>
          <a id = "ref_6"></a>[6] <a href = "https://arxiv.org/abs/2502.10259">MITO: A Millimeter-Wave Dataset and Simulator for Non-Line-of-Sight Perception</a>, Dodds et al., 2025 <br><br>
      </div>
    </div>
    <div class="margin-right-block"></div>
  </div>

</body>
</html>
