<html>
<head>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']]
  }
};
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
  body {
    margin: 0;
    padding: 0;
    background-color: #ffffff;
    color: #222;
    font-family: -apple-system, BlinkMacSystemFont, "Helvetica Neue", Helvetica, Arial, sans-serif;
    line-height: 1.6;
  }

  a:link,a:visited {
    color: #0e7862;
    text-decoration: none;
  }
  a:hover {
    color: #24b597;
  }

  /* Main page layout: centered content with a left sidebar outline */

  .content-margin-container {
    display: flex;
    width: 100%;
    justify-content: center;
    align-items: flex-start;
    box-sizing: border-box;
    margin-bottom: 16px; /* reduced spacing */
  }

  .main-content-block {
    width: 60%;
    max-width: 900px;
    box-sizing: border-box;
    padding: 0 12px;   /* reduced from 16px */
    background-color: #ffffff;
    margin-top: 10px;
    margin-bottom: 10px;
  }

  .margin-left-block {
    font-size: 14px;
    width: 180px;
    max-width: 180px;
    box-sizing: border-box;
    padding: 0 12px;
  }

  .margin-right-block {
    font-size: 13px;
    width: 220px;
    max-width: 220px;
    box-sizing: border-box;
    padding: 0 6px; /* reduced */
    color: #555;
  }

  /* Keep the outline as a fixed left sidebar like the original */

  #intro .margin-left-block {
    position: fixed;
    top: max(20%, 120px);
    left: 10px;
    max-width: 180px;
  }

  /* Make sure other sections don't shift from the fixed outline width */
  #prior_work .margin-left-block,
  #methods .margin-left-block,
  #discussion .margin-left-block,
  #conclusion .margin-left-block,
  #citations .margin-left-block {
    visibility: hidden;
  }

  .main-content-block.header-block {
    margin-top: 20px; /* was 40px */
    margin-bottom: 0px;
  }

  table.header {
    font-weight: 400;
    font-size: 17px;
    width: 100%;
    border-collapse: collapse;
  }
  table td, table td * {
    vertical-align: middle;
    position: relative;
  }

  table.paper-code-tab {
    display: none;
  }

  h1 {
    font-size: 28px;
    margin-top: 20px;  /* reduced */
    margin-bottom: 8px; /* reduced */
    font-weight: 600;
  }

  h3 {
    margin-top: 14px;
    margin-bottom: 6px;
  }

  img {
    max-width: 100%;
    height: auto;
    display: block;
    margin: 0 auto;
  }

  .my-video {
    max-width: 100%;
    height: auto;
    display: block;
    margin: 0 auto;
  }

  p {
    margin-top: 6px;
    margin-bottom: 6px; /* tighter paragraphs */
  }

  .mathjax-mobile, .mathml-non-mobile { display: none; }
  .show-mathml .mathml-non-mobile { display: block; }
  .show-mathjax .mathjax-mobile { display: block; }

  .vid-mobile, .vid-non-mobile { display: none; }
  .show-vid-mobile .vid-mobile { display: block; }
  .show-vid-non-mobile .vid-non-mobile { display: block; }

  hr {
    height: 1px;
    border: none;
    background-color: #ddd;
    margin: 12px 0; /* reduced */
    max-width: 900px;
  }

  div.hypothesis {
    width: 80%;
    max-width: 800px;
    background-color: #EEE;
    border: 1px solid #ccc;
    border-radius: 6px;
    font-family: "SFMono-Regular", Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
    font-size: 16px;
    text-align: center;
    margin: 16px auto; /* reduced */
    padding: 12px; /* reduced */
  }

  div.citation {
    font-size: 0.9em;
    background-color:#fff;
    padding: 6px; /* reduced */
    border-top: 1px solid #eee;
  }

  .fade-in-inline {
    position: absolute;
    text-align: center;
    margin: auto;
    -webkit-mask-image: linear-gradient(to right,
                                      transparent 0%,
                                      transparent 40%,
                                      black 50%,
                                      black 90%,
                                      transparent 100%);
    mask-image: linear-gradient(to right,
                                transparent 0%,
                                transparent 40%,
                                black 50%,
                                black 90%,
                                transparent 100%);
    -webkit-mask-size: 8000% 100%;
    mask-size: 8000% 100%;
    animation-name: sweepMask;
    animation-duration: 4s;
    animation-iteration-count: infinite;
    animation-timing-function: linear;
    animation-delay: -1s;
  }

  .fade-in2-inline {
    animation-delay: 1s;
  }

  .inline-div {
    position: relative;
    display: inline-block;
    vertical-align: top;
    width: 50px;
  }

  @keyframes sweepMask {
    0%   { -webkit-mask-position: 0% 0; mask-position: 0% 0; }
    100% { -webkit-mask-position: 100% 0; mask-position: 100% 0; }
  }

  @media (max-width: 900px) {
    .content-margin-container {
      justify-content: flex-start;
    }
    .main-content-block {
      width: calc(100% - 220px);
    }
  }

  @media (max-width: 700px) {
    #intro .margin-left-block {
      position: static;
      top: auto;
      left: auto;
    }
    #prior_work .margin-left-block,
    #methods .margin-left-block,
    #discussion .margin-left-block,
    #conclusion .margin-left-block,
    #citations .margin-left-block {
      display: none;
    }
    .main-content-block {
      width: 100%;
      max-width: 100%;
      padding: 0 16px;
    }
    .margin-right-block {
      display: none;
    }
  }

  .project-header {
    display: flex;
    flex-direction: column;
    gap: 2px; /* reduced */
    padding-bottom: 4px;
  }

  .project-header .title-line {
    font-size: 32px;
    font-family: 'Courier New', Courier, monospace;
    margin-bottom: 2px; /* reduced */
  }

  .project-header .authors {
    font-size: 17px;
    display: flex;
    flex-direction: row;
    gap: 8px;
    align-items: center;
  }

  .project-header .subtitle {
    font-size: 18px;
    margin-top: 0px; /* reduced */
  }

</style>

<title>6.7960 Fall 2025 Project</title>
<meta property="og:title" content="6.7960 Fall 2025 Project" />
<meta charset="UTF-8">
</head>

<body>

  <!-- Header / title block -->
  <div class="content-margin-container">
    <div class="margin-left-block">
      <!-- empty here, outline lives in intro block -->
    </div>
    <div class="main-content-block header-block">
      <div class="project-header">
		<div class="title-line">mmWave 3D Reconstruction</div>
		<div class="authors">
			<a href="your_website">Christopher Liem</a>
			and
			<a href="your_partner's_website">Yibo Cheng</a>
		</div>

		<div class="subtitle">Final project for 6.7960, MIT</div>
	  </div>

      <hr>
    </div>
    <div class="margin-right-block">
      <!-- optional header margin note -->
    </div>
  </div>

  <div class="content-margin-container" id="intro">
    <div class="margin-left-block">
      <b style="font-size:16px">Outline</b><br><br>
      <a href="#intro">Introduction</a><br><br>
      <a href="#prior_work">Prior Work</a><br><br>
      <a href="#methods">Methods & Experiments</a><br><br>
      <a href="#discussion">Discussion</a><br><br>
      <a href="#conclusion">Conclusion</a><br><br>
    </div>
  </div>

  <div class="content-margin-container" id="intro-text">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
      <h1>Introduction</h1>
	  <hr>
      <p>
        A point cloud is a set of discrete data points in space that, when concatenated together, can be used to represent a 3D shape or object.
        As a result, point clouds have various roles in autonomous driving, medical imaging, robotic manipulation, and more. For example,
        autonomous vehicles typically utilize LiDAR sensors to generate point clouds for real-time 3D perception of their environment. In particular,
         point clouds provide precise information about the distance and shape of surrounding objects that computer vision techniques alone can't achieve.
         In combination with semantic-processing, this information allows the vehicle to recognize when it is in a dangerous situation and decide the best
         action to take.
      </p>
      <p>
        However, despite its widespread use in multiple industries, point clouds are often incomplete due to various reasons: missing object details,
        occlusion, sensor resolution, and more. This motivates the open-research field of 3D reconstruction where the missing regions of the incomplete
        point clouds are recovered.
      </p>
      <p>
        Another substantial research field is the application of millimeter waves, or mmWave for short. mmWaves are radio-frequency signals
        that offer massive bandwidths for communication, which companies such as Verizon exploit for their 5G network. More interestingly to us, mmWaves can travel
        through occlusions such as boxes, allowing us to generate point clouds of hidden objects. However, the distinct physical properties of mmWaves cause their measurements to be more sparse and noisy
         than visible-light sensors such as LiDARS, further motivating the field of point cloud completion using different sensors.
      </p>

      <p>
        In the following sections, we build off of current state-of-the-art methods for 3D reconstruction to produce a model
        that performs well on mmWave-generated point clouds and evaluate potential reasons that led to our results.
      </p>
    </div>
    <div class="margin-right-block">
      Margin note that clarifies some detail #main-content-block for intro section.
    </div>
  </div>

  <div class="content-margin-container" id="prior_work">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
      <h1>Prior Work</h1>
	  <hr>
      <p>
        Research on completing 3D shapes from partial point clouds spans a progression of methods, starting
        from geometric-based algorithms and evolving towards modern deep-learning approaches.
      </p>
      <p>
        A major field of geometric surface reconstruction methods are those that interpolate a point cloud using
        the existing geometry without any additional information. For example, Delaunay-based methods exploit
        the fact that the reconstructed triangulated surface can be formed by a subcomplex of the Delaunay triangulation, which
        connects a set of points with non-overlapping triangles such that no point is inside the circumscribed circle within any triangle. While these
        methods have provable geometric and topological guarantees, they require dense sampling of the desired surface which is often
        impractical for imperfect real-world data. This limitation motivates a transition toward learning-based approaches, which aim
        to infer missing geometry based on learned priors rather than strict analytical rules.
      </p>
      <p>
        More recently, transformer-based models such as PoinTr leverage attention mechanisms to capture the geometric relationships
        between points. The pipeline for PoinTr can be summarized into two key points:
        <ol>
          <li>
            The point clouds in a local region are condensed into a feature vector which is called a "Point Proxy." Thus, the input point cloud is converted
            into a sequence of Point Proxies which are then fed into the rest of the pipeline for processing.
          </li>
          <li>
            To solve point cloud completion as a set-to-set translation problem, an encoder-decoder architecture is used. Given a set of Point Proxies,
            or partial point clouds, we wish to output a set of complete point clouds. To achieve this, the encoder maps the Point Proxies into
            a latent space that hopefully preserves important features which the decoder uses to infer the missing points. Specifically,
            geometry-aware transformer blocks are used to model the pairwise geometric relations between points which are crucial for the
            decoder to reason about the missing regions. This allows the model to leverage the inductive biases of 3D geometric structures
            in point clouds.
          </li>
        </ol>
      </p>
      <p>
        When benchmarked using the L1 Chamfer Distance on the PCN dataset, it was shown that PoinTr had substantially better performance
        than other state-of-the-art methods such as TopNet and Point Cloud Completion Network, both of which are also learning-based approaches
        to the same problem. It is important to note, however, that all of these existing methods are able to produce reasonable outputs when given
        incomplete data.
      </p>
      <p>
        While prior models may perform well on vision-like partial point clouds, they don't generalize well to
        the sparse and noisy partials produced by mmWave measurements. Thus, we wish to build off of the recent
        advancements in learning-based 3D reconstruction to solve the domain-specific problem of mmWave-produced point cloud
        completion.
      </p>
    </div>
    <div class="margin-right-block"></div>
  </div>

  <div class="content-margin-container" id="methods">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
      <h1>Methods & Experiments</h1>
	  <hr>
    <p>
      We build upon PoinTr, a transformer-based point cloud completion model, and introduce several key modifications to adapt
      it to the unique physical characteristics of mmWave radar signals. Below, we describe our synthetic dataset generation pipeline, architectural changes, and density-aware loss.</p>


	  <h3>Synthetic Dataset</h3>
	  <p>
		To train a reconstruction network tailored to mmWave data, we create a synthetic dataset designed to mimic mmWave reflections. Our dataset consists of three publicly available 3D shape repositories:
		<ul>
			<li><b>OmniObject3D</b>: A large multi-view 3D dataset comprised of 6,000 scans of real objects in 190 categories.</li>
      <figure style="max-width: 400px; margin: 0 auto; text-align: center;">
        <img src="images/omniobject.png" alt="OmniObject3D Examples" style="max-width: 100%; height: auto;">
        <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
          OmniObject3D Examples
        </figcaption>
      </figure>
			<li><b>Toys4K-3D</b>: A dataset consisting of 4,179 3D objects in 105 categories that capture objects
			encountered by infants and children.</li>
      <figure style="max-width: 400px; margin: 0 auto; text-align: center;">
        <img src="images/toys4k.png" alt="Toys4K-3D Examples" style="max-width: 100%; height: auto;">
        <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
          Toys4K-3D Examples
        </figcaption>
      </figure>
			<li><b>Thingiverse (Objaverse)</b>: A subset of around 15k objects from the Thingiverse platform. Most
			contain untextured meshes, which is useful for learning shape priors. </li>
      <figure style="max-width: 400px; margin: 0 auto; text-align: center;">
        <img src="images/thingiverse.png" alt="Thingiverse Examples" style="max-width: 100%; height: auto;">
        <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
          Thingiverse Examples
        </figcaption>
      </figure>
		</ul>
	  </p>

    <p>
      Existing point cloud completion models are typically trained on vision-style partials from visible-light sensors such as RGB-D cameras or LiDAR. mmWave signals, however,
      produce sparse, specular, anisotropic reflections with significant noise and limited angular resolution. To close the domain gap, we encode mmWave physics into the training sampels.
    </p>

    We illustrate the preprocessing pipeline using a lion sample, assuming the radar is located above it.

    <figure style="max-width: 200px; margin: 0 auto; text-align: center;">
      <img src="images/complete_lion.png" alt="Lion" style="max-width: 100%; height: auto;">
      <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
        Lion
      </figcaption>
    </figure>

    <p>
      To embed mmWave specularity directly into these synthetic samples, we retain only the points that would (1) produce a specular return and (2) lie on the surface facing the radar.
      The first condition models the fact that mmWave reflections follow a specular law: a point is visible only if the incoming ray and reflected ray form equal angles with the
      surface normal in their shared plane. The second condition ensures the training partials only include geometry that physically “faces” the radar, since
      mmWave sensors rarely receive energy from back-facing or oblique surfaces. In other words, we create the set of partial observations $O$ such that
      \begin{align*}
      O &= \left\{ s_i \in F \,\middle|\, \theta_P(s_i) < \tau ,\; V(s_i) = 1 \right\}\\\\
      \theta_P(s_i)&=\min_{r\in P}|\arccos(n_i\cdot u_{r,i})|,\, u_{r,i}=\frac{p_r-s_i}{||p_r-s_i||}
      \end{align*}
    </p>

    <p>
      where $s_i$ is a point in the full 3D point cloud $F$ from the public datasets. $\theta_P(s_i)$ is the smallest angular mismatch
      between $s_i$ and any sensor position in $P$, the set of of possible radar positions, where $n_i$ is the surface normal at $s_i$ and
      $u_{r,i}$ is the unit vector pointing from $s_i$ to $p_r$. We choose the parameter $\tau$ to bound $\theta_P(s_i)$. $V(s_i)$ is 1 when $s_i$ lies on the radar facing surface and 0 otherwise.
    </p>

    <figure style="max-width: 200px; margin: 0 auto; text-align: center;">
      <img src="images/mmwave_mask.png" alt="mmwave_mask" style="max-width: 100%; height: auto;">
      <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
        Points that produce a specular return
      </figcaption>
    </figure>

    <figure style="max-width: 200px; margin: 0 auto; text-align: center;">
      <img src="images/los_mask.png" alt="los_mask" style="max-width: 100%; height: auto;">
      <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
        Points that lie on the surface facing the radar
      </figcaption>
    </figure>

    <figure style="max-width: 200px; margin: 0 auto; text-align: center;">
      <img src="images/after_mask.png" alt="after_mask" style="max-width: 100%; height: auto;">
      <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
        Point Cloud that satisfy the two conditions
      </figcaption>
    </figure>

    <p>
      Next, to simulate the limited resolution of mmWave radars, we remove the outlier points that are too far from their neighbors. Specifically, we cluster points based on their distance,
      if two points are within a distance of 0.03 meters, they are considered to be in the same cluster. The clusters with fewer than 100 points are filtered out.
    </p>

    <figure style="max-width: 200px; margin: 0 auto; text-align: center;">
      <img src="images/outlier_removal.png" alt="outlier removal" style="max-width: 100%; height: auto;">
      <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
        After outlier removal
      </figcaption>
    </figure>

	  <p>
		Finally, we inject Gaussian noise into the synthetic partial point clouds $O'$ to mimic the substantial noise present in real mmWave
		measurements.

    </p>

    <figure style="max-width: 400px; margin: 0 auto; text-align: center;">
      <img src="images/gaussian_noise.png" alt="add Gaussian Noise" style="max-width: 100%; height: auto;">
      <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
        Add Gaussian noise
      </figcaption>
    </figure>


    <h3>Full Reconstruction</h3>
    <p>
      The second change we make is the architecture of PoinTr. Originally, given a partial point cloud, PoinTr predicts the missing part and directly
      concatenate the prediction with the input partial to form the final output. However, this design assumes that the input partial is clean and reliable,
      which is not the case for mmWave measurements, as they are often noisy. As a result, the model never gets a chance to denoise the input partials.
    </p>

    <figure style="max-width: 400px; margin: 0 auto; text-align: center;">
      <img src="images/use_origianl_input.png" alt="use_origianl_input" style="max-width: 100%; height: auto;">
      <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
        Original PoinTr uses the input partial directly in the final output
      </figcaption>
    </figure>

    <p>
      To address this issue, we modify the model architecture, allowing it revisit the noisy partial input and predict the full point cloud directly.
      This way, the model has the flexibility to reinterpret and denoise the input partials, rather than being constrained
      to preserve them in the final output.
    </p>

    <figure style="max-width: 400px; margin: 0 auto; text-align: center;">
      <img src="images/full_reconstruction.png" alt="full_reconstruction" style="max-width: 100%; height: auto;">
      <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
        Full reconstruct the point cloud
      </figcaption>
    </figure>

    <h3>Density-aware Cost Function</h3>

    <p>
      During training, we noticed the prediction point cloud tended to have uneven point density, with some regions being overly dense while others were sparse.
      This is because the original Chamfer Distance loss used in PoinTr treats all points equally, regardless of their spatial distribution. If one area of the
      point cloud is too dense vs the ground truth (or too sparse), CD may not penalize it sufficiently.
		$$
			\mathcal{L}=\frac{1}{|\hat{F}|}\sum_{s_i\in \hat{F}}\min_{g\in F}||s_i-g||+\frac{1}{|F|}\sum_{g\in F}\min_{s_i\in \hat{F}}||g-s_i||
		$$
    where $\hat{F}$ is the predicted full point cloud and $F$ is the ground-truth full point cloud.

    To encourage more uniform point distributions, we apply a density-aware cost function that weights points based on their local density.
    $$
        d_{\mathrm{DCD}}(R, G) =
      \frac{1}{2} \left(
      \frac{1}{|R|} \sum_{r \in R}
      \left( 1 - \frac{1}{n_{\hat{g}}} e^{-\alpha \|r - \hat{g}\|_2} \right)
      +
      \frac{1}{|G|} \sum_{g \in G}
      \left( 1 - \frac{1}{n_{\hat{r}}} e^{-\alpha \|g - \hat{r}\|_2} \right)
      \right)
    $$
    where $\hat{g} = \min_{g \in G} \|r - g\|_2$ and $\hat{r} = \min_{r \in R} \|g - r\|_2$. $\alpha$ is a temperature scaler, $n_{g} = |R^g |$ and $n_{r} = |G^r |$.
    Intuitively, if a point is in a dense region (large $n_{\hat{g}}$ or $n_{\hat{r}}$), its contribution to the loss is down-weighted, encouraging the model to spread points more evenly.

	  </p>
    </div>
    <div class="margin-right-block"></div>
  </div>

  <div class="content-margin-container" id=".">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
      <h3>Experiments</h3>
      <p>
        To evaluate the performance of our model, we use real-world mmWave measurements of 61 objects from the MITO dataset. The dataset include daily objects with
		    diverse properties, both material and geometric, such as wooden v.s. metal and flat v.s. curved. To further
		    test real-world application, both line-of-sight and fully-obstructed mmWave data of each object are used.
      </p>



    </div>
    <div class="margin-right-block"></div>
  </div>

  <div class="content-margin-container" id="discussion">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
      <h1>Discussion</h1>
	  <hr>
    <h3>Results</h3>
    <p>
        Following prior work, we use Chamfer Distance and F-Score as the main evaluation metric. Lower CD indicates better reconstruction quality. Higher F-Score indicates better reconstruction quality. Table shows the quantitative performance of PoinTr and our model on synthetic dataset and real-world mmWave data.
      </p>

      <table border="1" cellpadding="8" cellspacing="0">
        <tr>
          <th rowspan="2"></th>
          <th colspan="2">Synthetic Dataset</th>
          <th colspan="2">Real World Dataset</th>
        </tr>

        <tr>
          <th>Chamfer Distance</th>
          <th>F-Score</th>
          <th>Chamfer Distance</th>
          <th>F-Score</th>
        </tr>

        <tr>
          <th>PoinTr</th>
          <td>0.071</td>
          <td>70%</td>
          <td>0.104</td>
          <td>62%</td>
        </tr>

        <tr>
          <th>Our Model</th>
          <td>0.034</td>
          <td>91%</td>
          <td>0.069</td>
          <td>75%</td>
        </tr>
      </table>

      <figure style="max-width: 400px; margin: 0 auto; text-align: center;">
        <img src="images/visualization.png" alt="visualization" style="max-width: 100%; height: auto;">
        <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
          Qualitative Comparison between PoinTr and Our Model
        </figcaption>
      </figure>

      <p>
        Our model outperforms PoinTr on both synthetic and real-world datasets, demonstrating the effectiveness of our modifications in adapting to mmWave data.

      </p>
      <p>
        We further inverstigate why our model outperforms PoinTr by analyzing the latent space of both models. We visualize
        the latent space using PCA. Specifically, we take the output of the transformer encoder as the latent representation of the input partial point cloud.
        Then we apply PCA to reduce the dimension to 2D for visualization. Figure shows the PCA visualization of the latent space of PoinTr (ckpt_a) and our model (ckpt_b).
      </p>

      <figure style="max-width: 400px; margin: 0 auto; text-align: center;">
        <img src="images/encoder_pca.png" alt="encoder_pcader_pca" style="max-width: 100%; height: auto;">
        <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
          PCA Analysis on Encoder Output
        </figcaption>
      </figure>

      <p>
        We also visualize the PCA analysis on the decoder output of both models. The decoder output is the set of point proxies that are used to generate the final point cloud.
        Figure shows the PCA visualization of the decoder output of PoinTr (ckpt_a) and our model (ckpt_b).
      </p>

      <figure style="max-width: 400px; margin: 0 auto; text-align: center;">
        <img src="images/decoder_pca.png" alt="decoder_pcader_pca" style="max-width: 100%; height: auto;">
        <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
          PCA Analysis on Decoder Output
        </figcaption>
      </figure>

      <p>
        As shown in both figures, our model maps each object into different regions in the latent space, indicating that the model learns to distinguish different objects effectively.
        In contrast, PoinTr's latent space shows significant overlap among different objects, suggesting that it struggles to differentiate between them. This improved separation in the latent space likely contributes to our model's
        superior reconstruction performance on mmWave data.
      </p>

      <p>
        Despite our improved performance in comparison to PoinTr, our model still has limitations in accuracy, specifically with small objects that have irregular shape. For the case of small objects,
        we tested our model on a large clamp, a medium clamp, and a small clamp. The real mmWave point cloud of the clamp was substantially lower resolution than the
        one for the large clamp, which resulted in a worse input to be fed to our model. As a result, our model's performance deteriorates as our test object gets smaller. This can be visualized in the figure
        below.
      </p>
      <b>TODO: maybe give chamfer distances of the clamps + visualize clamp completed PC</b>
      <p>
        Furthermore, our model
      </p>

      <p>
        Thus, our model struggles to reconstruct geometries for objects that are small and have irregular shapes.
      </p>

      <b>TODO: significance is that we were able to apply specialize existing state of the art models into different sensor domains,
        allowing us to fine tune models for specific applications or smth
      </b>

      <b>hammer: handle and head different materials -> different reflection so point cloud detached -> can't account for this</b>
    </div>
    <div class="margin-right-block"></div>
  </div>

  <div class="content-margin-container" id="conclusion">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
      <h1>Conclusion</h1>
	  <hr>
      <q>
        In this project, we set out to adapt state-of-the-art point cloud completion techniques to the fundamentally different sensing regime of mmWave radar. Unlike vision-based sensors,
        mmWave systems generate highly sparse, noisy, and specular measurements, making direct application of existing models ineffective. To address this challenge, we introduced a physics-aware
        synthetic data pipeline, a full-reconstruction architecture that enables implicit denoising, and a density-aware loss that promotes more uniform point distributions.

        Through extensive experiments on both synthetic data and real-world measurements from the MITO dataset, our model consistently outperforms PoinTr across Chamfer Distance and F-Score metrics.
        Qualitative results further demonstrate improved geometric fidelity, especially in cases where mmWave sparsity is severe. Latent-space analysis suggests that our architectural changes lead to
        more structured and discriminative feature representations, enabling the model to better distinguish object categories and infer missing geometry.

        Beyond performance gains, this work contributes to the broader understanding of deep learning in several ways:
        <ul>
          <li>It highlights the importance of incorporating domain-specific physics into data generation and model design, rather than relying solely on generic architectures.</li>

          <li>It provides a concrete demonstration of how deep learning models behave under severe domain shift: models trained on vision-like data fail to generalize to mmWave inputs, whereas models equipped with the appropriate inductive biases can successfully transfer across sensing modalities.</li>

          <li>It illustrates that architectural and loss-function choices directly reshape the latent manifold: by enforcing full reconstruction and density-aware optimization, the model learns latent clusters that better reflect object categories and geometric relationships.</li>

          <li>It reinforces the broader understanding that deep learning is not just about scaling data or model size; rather, the alignment between data characteristics, sensor physics, and model assumptions fundamentally determines the quality of learned representations.</li>
        </ul>


        While our approach significantly improves reconstruction quality, challenges remain, especially for small or irregular objects that produce weak mmWave returns. Future work may integrate richer
        generative priors, differentiable radar simulation, or multi-sensor fusion to further enhance robustness.

        Overall, our findings underscore a key lesson: deep learning models are not merely pattern matchers but systems whose effectiveness depends critically on the inductive biases we embed into their data, architecture, and objectives. By aligning these biases with the physics of mmWave sensing,
        we move toward reliable 3D perception beyond the domain of traditional vision sensors.
      </q>
    </div>
    <div class="margin-right-block"></div>
  </div>

  <div class="content-margin-container" id="citations">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
      <div class='citation' id="references" style="height:auto"><br>
        <span style="font-size:16px">References:</span><br><br>
        <a id="ref_1"></a>[1] <a href="https://en.wikipedia.org/wiki/Allegory_of_the_cave">Allegory of the Cave</a>, Plato, c. 375 BC<br><br>
        <a id="ref_2"></a>[2] <a href="">A Human-Level AGI</a>, OpenAI, 2025<br><br>
      </div>
    </div>
    <div class="margin-right-block"></div>
  </div>

</body>
</html>
