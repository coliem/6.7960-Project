<html>
<head>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']]
  }
};
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
  body {
    margin: 0;
    padding: 0;
    background-color: #ffffff;
    color: #222;
    font-family: -apple-system, BlinkMacSystemFont, "Helvetica Neue", Helvetica, Arial, sans-serif;
    line-height: 1.6;
  }

  a:link,a:visited {
    color: #0e7862;
    text-decoration: none;
  }
  a:hover {
    color: #24b597;
  }

  /* Main page layout: centered content with a left sidebar outline */

  .content-margin-container {
    display: flex;
    width: 100%;
    justify-content: center;
    align-items: flex-start;
    box-sizing: border-box;
    margin-bottom: 16px; /* reduced spacing */
  }

  .main-content-block {
    width: 60%;
    max-width: 900px;
    box-sizing: border-box;
    padding: 0 12px;   /* reduced from 16px */
    background-color: #ffffff;
    margin-top: 10px;
    margin-bottom: 10px;
  }

  .margin-left-block {
    font-size: 14px;
    width: 180px;
    max-width: 180px;
    box-sizing: border-box;
    padding: 0 12px;
  }

  .margin-right-block {
    font-size: 13px;
    width: 220px;
    max-width: 220px;
    box-sizing: border-box;
    padding: 0 6px; /* reduced */
    color: #555;
  }

  /* Keep the outline as a fixed left sidebar like the original */

  #intro .margin-left-block {
    position: fixed;
    top: max(20%, 120px);
    left: 10px;
    max-width: 180px;
  }

  /* Make sure other sections don't shift from the fixed outline width */
  #prior_work .margin-left-block,
  #methods .margin-left-block,
  #discussion .margin-left-block,
  #conclusion .margin-left-block,
  #citations .margin-left-block {
    visibility: hidden;
  }

  .main-content-block.header-block {
    margin-top: 20px; /* was 40px */
    margin-bottom: 0px;
  }

  table.header {
    font-weight: 400;
    font-size: 17px;
    width: 100%;
    border-collapse: collapse;
  }
  table td, table td * {
    vertical-align: middle;
    position: relative;
  }

  table.paper-code-tab {
    display: none;
  }

  h1 {
    font-size: 28px;
    margin-top: 20px;  /* reduced */
    margin-bottom: 8px; /* reduced */
    font-weight: 600;
  }

  h3 {
    margin-top: 14px;
    margin-bottom: 6px;
  }

  img {
    max-width: 100%;
    height: auto;
    display: block;
    margin: 0 auto;
  }

  .my-video {
    max-width: 100%;
    height: auto;
    display: block;
    margin: 0 auto;
  }

  p {
    margin-top: 6px;
    margin-bottom: 6px; /* tighter paragraphs */
  }

  .mathjax-mobile, .mathml-non-mobile { display: none; }
  .show-mathml .mathml-non-mobile { display: block; }
  .show-mathjax .mathjax-mobile { display: block; }

  .vid-mobile, .vid-non-mobile { display: none; }
  .show-vid-mobile .vid-mobile { display: block; }
  .show-vid-non-mobile .vid-non-mobile { display: block; }

  hr {
    height: 1px;
    border: none;
    background-color: #ddd;
    margin: 12px 0; /* reduced */
    max-width: 900px;
  }

  div.hypothesis {
    width: 80%;
    max-width: 800px;
    background-color: #EEE;
    border: 1px solid #ccc;
    border-radius: 6px;
    font-family: "SFMono-Regular", Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
    font-size: 16px;
    text-align: center;
    margin: 16px auto; /* reduced */
    padding: 12px; /* reduced */
  }

  div.citation {
    font-size: 0.9em;
    background-color:#fff;
    padding: 6px; /* reduced */
    border-top: 1px solid #eee;
  }

  .fade-in-inline {
    position: absolute;
    text-align: center;
    margin: auto;
    -webkit-mask-image: linear-gradient(to right,
                                      transparent 0%,
                                      transparent 40%,
                                      black 50%,
                                      black 90%,
                                      transparent 100%);
    mask-image: linear-gradient(to right,
                                transparent 0%,
                                transparent 40%,
                                black 50%,
                                black 90%,
                                transparent 100%);
    -webkit-mask-size: 8000% 100%;
    mask-size: 8000% 100%;
    animation-name: sweepMask;
    animation-duration: 4s;
    animation-iteration-count: infinite;
    animation-timing-function: linear;
    animation-delay: -1s;
  }

  .fade-in2-inline {
    animation-delay: 1s;
  }

  .inline-div {
    position: relative;
    display: inline-block;
    vertical-align: top;
    width: 50px;
  }

  @keyframes sweepMask {
    0%   { -webkit-mask-position: 0% 0; mask-position: 0% 0; }
    100% { -webkit-mask-position: 100% 0; mask-position: 100% 0; }
  }

  @media (max-width: 900px) {
    .content-margin-container {
      justify-content: flex-start;
    }
    .main-content-block {
      width: calc(100% - 220px);
    }
  }

  @media (max-width: 700px) {
    #intro .margin-left-block {
      position: static;
      top: auto;
      left: auto;
    }
    #prior_work .margin-left-block,
    #methods .margin-left-block,
    #discussion .margin-left-block,
    #conclusion .margin-left-block,
    #citations .margin-left-block {
      display: none;
    }
    .main-content-block {
      width: 100%;
      max-width: 100%;
      padding: 0 16px;
    }
    .margin-right-block {
      display: none;
    }
  }

  .project-header {
    display: flex;
    flex-direction: column;
    gap: 2px; /* reduced */
    padding-bottom: 4px;
  }

  .project-header .title-line {
    font-size: 32px;
    font-family: 'Courier New', Courier, monospace;
    margin-bottom: 2px; /* reduced */
  }

  .project-header .authors {
    font-size: 17px;
    display: flex;
    flex-direction: row;
    gap: 8px;
    align-items: center;
  }

  .project-header .subtitle {
    font-size: 18px;
    margin-top: 0px; /* reduced */
  }

</style>

<title>6.7960 Fall 2025 Project</title>
<meta property="og:title" content="6.7960 Fall 2025 Project" />
<meta charset="UTF-8">
</head>

<body>

  <!-- Header / title block -->
  <div class="content-margin-container">
    <div class="margin-left-block">
      <!-- empty here, outline lives in intro block -->
    </div>
    <div class="main-content-block header-block">
      <div class="project-header">
		<div class="title-line">mmWave 3D Reconstruction</div>
		<div class="authors">
			<a href="your_website">Christopher Liem</a>
			and
			<a href="your_partner's_website">Yibo Cheng</a>
		</div>

		<div class="subtitle">Final project for 6.7960, MIT</div>
	  </div>

      <hr>
    </div>
    <div class="margin-right-block">
      <!-- optional header margin note -->
    </div>
  </div>

  <div class="content-margin-container" id="intro">
    <div class="margin-left-block">
      <b style="font-size:16px">Outline</b><br><br>
      <a href="#intro">Introduction</a><br><br>
      <a href="#prior_work">Prior Work</a><br><br>
      <a href="#methods">Methods & Experiments</a><br><br>
      <a href="#discussion">Discussion</a><br><br>
      <a href="#conclusion">Conclusion</a><br><br>
    </div>
  </div>

  <div class="content-margin-container" id="intro-text">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
      <h1>Introduction</h1>
	  <hr>
      <p>
        A point cloud is a set of discrete points in 3D space that, when concatenated together, can represent the geometry of an object or scene.
        Point clouds play a central role in autonomous driving, medical imaging, robotic manipulation, and other applications. For example,
        autonomous vehicles typically utilize LiDAR sensors to generate point clouds for real-time 3D perception of their environment. These
        point clouds provide precise information about the distance and shape of surrounding objects that purely image-based techniques can't achieve.
        Combined with semantic understanding, this information allows the vehicle to recognize potentially dangerous situations and decide the best
        action to take.
      </p>
      <p>
        In practice, point clouds are often incomplete due to occlusions, limited sensor resolution, missing object parts,
        or noise in the measurement process. This motivates the problem of <em>point cloud completion</em>, where the goal is
        to reconstruct the full underlying 3D shape from a partial observation. Point cloud completion has become an active
        area of research, with methods ranging from classical geometric reconstruction to modern deep learning approaches.
      </p>
      <p>
        In parallel, there has been growing interest in millimeter waves, or mmWave for short. mmWaves are radio-frequency signals
        that offer massive bandwidths for communication, which companies such as Verizon exploit for their 5G network. More interestingly to us, mmWaves can travel
        through occlusions such as cardboard or fabric, allowing us to obtain point clouds of objects that are
        partially hidden or completely occluded in the visible spectrum. However, mmWave measurements exhibit very different
        physical characteristics from visible-light sensors: they are highly sparse, specular, and noisy, with limited angular
        resolution. As a result, deep models trained on vision-like partial point clouds fail to generalize directly to mmWave data.
      </p>

      <div class="hypothesis">
        <b>Central hypothesis.</b><br>
        We hypothesize that adapting point cloud completion models to mmWave sensing requires explicitly aligning the model’s
        inductive biases with mmWave physics. Concretely, we propose that
        <ul>
          <li>A physics-aware synthetic data pipeline that encodes mmWave reflection geometry,</li>
          <li>A full-reconstruction architecture that can denoise and reinterpret noisy partial inputs, and</li>
          <li>A density-aware loss that regularizes local point distributions</li>
        </ul>
        together will substantially improve reconstruction quality on real mmWave data compared to a state-of-the-art baseline
        designed for vision-style partials.
      </div>

      <p>
        In this project, we build on PoinTr, a transformer-based point cloud completion model, and adapt it to the mmWave domain.
        We introduce a synthetic dataset that incorporates mmWave reflection geometry, modify the architecture to predict the full
        point cloud (rather than only the missing part), and replace the standard Chamfer Distance loss with a density-aware variant.
        We then evaluate our model on both synthetic data and real-world mmWave measurements, analyze the learned latent
        representations, and discuss how our findings shed light on the role of inductive bias and latent manifolds in deep
        learning for 3D perception.
      </p>
    </div>
    <div class="margin-right-block">
      Margin note that clarifies some detail #main-content-block for intro section.
    </div>
  </div>

  <div class="content-margin-container" id="prior_work">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
      <h1>Prior Work</h1>
	  <hr>
      <p>
        Research on completing 3D shapes from partial point clouds spans a progression of methods, beginning
        from geometric-based algorithms and evolving towards modern deep-learning approaches.
      </p>
      <p>
        A major field of geometric surface reconstruction methods are those that interpolate a point cloud using
        the existing geometry without any additional information. For example, Delaunay-based methods exploit
        the fact that the reconstructed triangulated surface can be formed by a subcomplex of the Delaunay triangulation, which
        connects a set of points with non-overlapping triangles such that no point is inside the circumscribed circle within any triangle. While these
        methods have provable geometric and topological guarantees, they require dense sampling of the desired surface which is often
        impractical for imperfect real-world data. This limitation motivates a transition toward learning-based approaches, which aim
        to infer missing geometry based on learned priors rather than strict analytical rules.
      </p>
      <p>
        More recently, transformer-based models such as PoinTr leverage attention mechanisms to capture the geometric relationships
        between points. The pipeline for PoinTr can be summarized into two key points:
        <ol>
          <li>
            The point clouds in a local region are condensed into a feature vector which is called a "Point Proxy." Thus, the input point cloud is converted
            into a sequence of Point Proxies which are then fed into the rest of the pipeline for processing.
          </li>
          <li>
            To solve point cloud completion as a set-to-set translation problem, an encoder-decoder architecture is used. Given a set of Point Proxies,
            or partial point clouds, we wish to output a set of complete point clouds. To achieve this, the encoder maps the Point Proxies into
            a latent space that hopefully preserves important features which the decoder uses to infer the missing points. Specifically,
            geometry-aware transformer blocks are used to model the pairwise geometric relations between points which are crucial for the
            decoder to reason about the missing regions. This allows the model to leverage the inductive biases of 3D geometric structures
            in point clouds.
          </li>
        </ol>
      </p>
      <p>
        When benchmarked using the L1 Chamfer Distance on the PCN dataset, it was shown that PoinTr had substantially better performance
        than other state-of-the-art methods such as TopNet and Point Cloud Completion Network, both of which are also learning-based approaches
        to the same problem. It is important to note, however, that all of these existing methods are able to produce reasonable outputs when given
        incomplete data.
      </p>
      <p>
        While prior models may perform well on vision-like partial point clouds, they don't generalize well to
        the sparse and noisy partials produced by mmWave measurements. Thus, we wish to build off of the recent
        advancements in learning-based 3D reconstruction to solve the domain-specific problem of mmWave-produced point cloud
        completion.
      </p>
    </div>
    <div class="margin-right-block"></div>
  </div>

  <div class="content-margin-container" id="methods">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
      <h1>Methods & Experiments</h1>
	  <hr>
    <p>
      We build upon PoinTr, a transformer-based point cloud completion model, and introduce several key modifications to adapt
      it to the unique physical characteristics of mmWave radar signals. Below, we describe our synthetic dataset generation pipeline, architectural changes, and density-aware loss.</p>


	  <h3>Synthetic Dataset for mmWave-Style Partials</h3>
	  <p>
		To train a reconstruction network tailored to mmWave data, we create a synthetic dataset designed to mimic mmWave reflections. Our dataset consists of three publicly available 3D shape repositories:
		<ul>
			<li><b>OmniObject3D</b>: A large multi-view 3D dataset comprised of roughly 6,000 scans of real objects in 190 categories.</li>
      <figure style="max-width: 400px; margin: 0 auto; text-align: center;">
        <img src="images/omniobject.png" alt="OmniObject3D Examples" style="max-width: 100%; height: auto;">
        <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
          Figure 1: Example objects from the OmniObject3D dataset
        </figcaption>
      </figure>
			<li><b>Toys4K-3D</b>: A dataset of 4,179 3D objects in 105 categories, focusing on toys and household objects commonly
          encountered by infants and children.</li>
      <figure style="max-width: 500px; margin: 0 auto; text-align: center;">
        <img src="images/toys4k.png" alt="Toys4K-3D Examples" style="max-width: 100%; height: auto;">
        <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
          Figure 2: Example objects from the Toys4K-3D dataset
        </figcaption>
      </figure>
			<li><b>Thingiverse (Objaverse)</b>: A subset of around 15k objects from the Thingiverse platform, mostly
          consisting of untextured meshes. These objects are useful for learning diverse shape priors. </li>
      <figure style="max-width: 500px; margin: 0 auto; text-align: center;">
        <img src="images/thingiverse.png" alt="Thingiverse Examples" style="max-width: 100%; height: auto;">
        <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
          Figure 3: Example meshes from the Thingiverse subset
        </figcaption>
      </figure>
		</ul>
	  </p>

    <p>
      Existing point cloud completion models are typically trained on vision-style partials from visible-light sensors such as RGB-D cameras or LiDAR. mmWave signals, however,
      produce sparse, specular, anisotropic reflections with significant noise and limited angular resolution. To close the domain gap, we encode mmWave physics into our synthetic training samples.
    </p>

    <p>
    We illustrate the preprocessing pipeline using a lion sample, assuming the radar is located above it.
    </p>

    <figure style="max-width: 300px; margin: 0 auto; text-align: center;">
      <img src="images/complete_lion.png" alt="Lion" style="max-width: 100%; height: auto;">
      <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
        Figure 4: Example complete point cloud of a lion mesh
      </figcaption>
    </figure>

    <p>
      To embed mmWave specularity directly into these synthetic samples, we retain only the points that would (1) produce a specular return and (2) lie on the surface facing the radar.
      The first condition models the fact that mmWave reflections follow a specular law: a point is visible only if the incoming ray and reflected ray form equal angles with the
      surface normal in their shared plane. The second condition ensures the training partials only include geometry that physically “faces” the radar, since
      mmWave sensors rarely receive energy from back-facing or oblique surfaces.
    </p>
    <p>
      Formally, we define the set of partial observations $O$ as
    </p>

    <p>
        $$
        O = \left\{ s_i \in F \,\middle|\, \theta_P(s_i) < \tau,\, V(s_i) = 1 \right\}
        $$
        where $F$ is the full 3D point cloud, $s_i$ is a point in $F$, and
        $$
        \theta_P(s_i) = \min_{p_r \in P} \left|\arccos(n_i \cdot u_{r,i})\right|,\quad
        u_{r,i} = \frac{p_r - s_i}{\lVert p_r - s_i \rVert_2}.
        $$
        Here $P$ is the set of possible radar positions, $n_i$ is the surface normal at $s_i$, $u_{r,i}$ is the unit vector pointing from $s_i$ to $p_r$, and $\tau$ is a threshold on
        the angular mismatch. The visibility indicator $V(s_i)$ is 1 when $s_i$ lies on the radar-facing surface and 0 otherwise.
      </p>

    <figure style="max-width: 400px; margin: 0 auto; text-align: center;">
      <img src="images/mmwave_mask.png" alt="mmwave_mask" style="max-width: 100%; height: auto;">
      <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
        Figure 5: Points that satisfy the mmWave specular reflection condition
      </figcaption>
    </figure>

    <figure style="max-width: 400px; margin: 0 auto; text-align: center;">
      <img src="images/los_mask.png" alt="los_mask" style="max-width: 100%; height: auto;">
      <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
        Figure 6: Points that lie on the surface facing the radar (line-of-sight constraint)
      </figcaption>
    </figure>

    <figure style="max-width: 400px; margin: 0 auto; text-align: center;">
      <img src="images/after_mask.png" alt="after_mask" style="max-width: 100%; height: auto;">
      <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
        Figure 7: Resulting partial point cloud after applying both constraints
      </figcaption>
    </figure>

    <p>
      Next, to simulate the limited resolution of mmWave radars, we remove the outlier points that are too isolated from their neighbors. Specifically, we cluster points based on their Euclidean distance:
      if two points are within 0.03 meters, they are assigned to the same cluster. Clusters with fewer than 100 points are filtered out. This step mimics the fact
        that very small or isolated returns are often lost in real mmWave measurements.
    </p>

    <figure style="max-width: 400px; margin: 0 auto; text-align: center;">
      <img src="images/outlier_removal.png" alt="outlier removal" style="max-width: 100%; height: auto;">
      <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
        Figure 8: Partial point cloud after outlier removal and cluster filtering
      </figcaption>
    </figure>

	  <p>
		  Finally, we inject Gaussian noise into the synthetic partial point clouds $O'$ to model the substantial noise present in
      real mmWave measurements.

    </p>

    <figure style="max-width: 500px; margin: 0 auto; text-align: center;">
      <img src="images/gaussian_noise.png" alt="add Gaussian Noise" style="max-width: 100%; height: auto;">
      <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
        Figure 9: Synthetic mmWave-style partial with added Gaussian noise
      </figcaption>
    </figure>


    <h3>Full Reconstruction</h3>
    <p>
      Next, we modify the architecture of PoinTr. Originally, given a partial point cloud, PoinTr predicts the missing part and directly
      concatenates the prediction with the input partial to produce the final output. However, this design assumes that the input partial is clean and reliable,
      which is not the case for mmWave measurements, as they are often noisy. As a result, the model never gets a chance to denoise the input partials.
    </p>

    <figure style="max-width: 400px; margin: 0 auto; text-align: center;">
      <img src="images/use_origianl_input.png" alt="use_origianl_input" style="max-width: 100%; height: auto;">
      <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
        Original PoinTr uses the input partial directly in the final output
      </figcaption>
    </figure>

    <p>
      To address this issue, we modify the model architecture, allowing it to revisit the noisy partial inputs and predict the full point cloud directly.
      This way, the model has the flexibility to reinterpret and denoise the input partials, rather than being constrained
      to preserve them in the final output.
    </p>

    <figure style="max-width: 400px; margin: 0 auto; text-align: center;">
      <img src="images/full_reconstruction.png" alt="full_reconstruction" style="max-width: 100%; height: auto;">
      <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
        Full reconstruct the point cloud
      </figcaption>
    </figure>

    <h3>Density-aware Cost Function</h3>

    <p>
      During training, we noticed that the predicted point cloud tends to have uneven point density, with some regions being overly dense while others were sparse.
      This is because the original Chamfer Distance (CD) loss used in PoinTr treats all points equally, regardless of their spatial distribution. For reference,
      the Chamfer Distance is given by
		$$
			\mathcal{L}=\frac{1}{|\hat{F}|}\sum_{s_i\in \hat{F}}\min_{g\in F}||s_i-g||+\frac{1}{|F|}\sum_{g\in F}\min_{s_i\in \hat{F}}||g-s_i||
		$$
    where $\hat{F}$ is the predicted full point cloud and $F$ is the ground-truth full point cloud. If one area of the
      point cloud is too dense (or sparse) vs the ground truth, then CD may not penalize it sufficiently.

    To encourage more uniform point distributions, we apply a density-aware cost function that weights points based on their local density.
    $$
        d_{\mathrm{DCD}}(R, G) =
      \frac{1}{2} \left(
      \frac{1}{|R|} \sum_{r \in R}
      \left( 1 - \frac{1}{n_{\hat{g}}} e^{-\alpha \|r - \hat{g}\|_2} \right)
      +
      \frac{1}{|G|} \sum_{g \in G}
      \left( 1 - \frac{1}{n_{\hat{r}}} e^{-\alpha \|g - \hat{r}\|_2} \right)
      \right)
    $$
    where $\hat{g} = \min_{g \in G} \|r - g\|_2$ and $\hat{r} = \min_{r \in R} \|g - r\|_2$. $\alpha$ is a temperature scaler, $n_{g} = |R^g |$ and $n_{r} = |G^r |$.
    Intuitively, if a point is in a dense region (large $n_{\hat{g}}$ or $n_{\hat{r}}$), its contribution to the loss is down-weighted, encouraging the model to spread points more evenly.

	  </p>
    </div>
    <div class="margin-right-block"></div>
  </div>

  <div class="content-margin-container" id=".">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
      <h3>Experiments</h3>
      <p>
        To evaluate the performance of our model, we use real-world mmWave measurements of 61 objects from the MITO dataset. The dataset include daily objects with
		    diverse properties, both material and geometric, such as wooden v.s. metal and flat v.s. curved. To further
		    test real-world application, both line-of-sight and fully-obstructed mmWave data of each object are used.
      </p>



    </div>
    <div class="margin-right-block"></div>
  </div>

  <div class="content-margin-container" id="discussion">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
      <h1>Discussion</h1>
	  <hr>
    <h3>Results</h3>
    <p>
        Following prior work, we use Chamfer Distance and F-Score as the main evaluation metric. Lower CD indicates better reconstruction quality. Higher F-Score indicates better reconstruction quality. Table shows the quantitative performance of PoinTr and our model on synthetic dataset and real-world mmWave data.
      </p>

      <table border="1" cellpadding="8" cellspacing="0">
        <tr>
          <th rowspan="2"></th>
          <th colspan="2">Synthetic Dataset</th>
          <th colspan="2">Real World Dataset</th>
        </tr>

        <tr>
          <th>Chamfer Distance</th>
          <th>F-Score</th>
          <th>Chamfer Distance</th>
          <th>F-Score</th>
        </tr>

        <tr>
          <th>PoinTr</th>
          <td>0.071</td>
          <td>70%</td>
          <td>0.104</td>
          <td>62%</td>
        </tr>

        <tr>
          <th>Our Model</th>
          <td>0.034</td>
          <td>91%</td>
          <td>0.069</td>
          <td>75%</td>
        </tr>
      </table>

      <figure style="max-width: 400px; margin: 0 auto; text-align: center;">
        <img src="images/visualization.png" alt="visualization" style="max-width: 100%; height: auto;">
        <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
          Qualitative Comparison between PoinTr and Our Model
        </figcaption>
      </figure>

      <p>
        Our model outperforms PoinTr on both synthetic and real-world datasets, demonstrating the effectiveness of our modifications in adapting to mmWave data.

      </p>
      <p>
        We further inverstigate why our model outperforms PoinTr by analyzing the latent space of both models. We visualize
        the latent space using PCA. Specifically, we take the output of the transformer encoder as the latent representation of the input partial point cloud.
        Then we apply PCA to reduce the dimension to 2D for visualization. Figure shows the PCA visualization of the latent space of PoinTr (ckpt_a) and our model (ckpt_b).
      </p>

      <figure style="max-width: 400px; margin: 0 auto; text-align: center;">
        <img src="images/encoder_pca.png" alt="encoder_pcader_pca" style="max-width: 100%; height: auto;">
        <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
          PCA Analysis on Encoder Output
        </figcaption>
      </figure>

      <p>
        We also visualize the PCA analysis on the decoder output of both models. The decoder output is the set of point proxies that are used to generate the final point cloud.
        Figure shows the PCA visualization of the decoder output of PoinTr (ckpt_a) and our model (ckpt_b).
      </p>

      <figure style="max-width: 400px; margin: 0 auto; text-align: center;">
        <img src="images/decoder_pca.png" alt="decoder_pcader_pca" style="max-width: 100%; height: auto;">
        <figcaption style="font-size: 0.9rem; color: #555; margin-top: 0.35rem;">
          PCA Analysis on Decoder Output
        </figcaption>
      </figure>

      <p>
        As shown in both figures, our model maps each object into different regions in the latent space, indicating that the model learns to distinguish different objects effectively.
        In contrast, PoinTr's latent space shows significant overlap among different objects, suggesting that it struggles to differentiate between them. This improved separation in the latent space likely contributes to our model's
        superior reconstruction performance on mmWave data.
      </p>

      <p>
        Despite our improved performance in comparison to PoinTr, our model still has limitations in accuracy, specifically with small objects that have irregular shape. For the case of small objects,
        we tested our model on a large clamp, a medium clamp, and a small clamp. The real mmWave point cloud of the clamp was substantially lower resolution than the
        one for the large clamp, which resulted in a worse input to be fed to our model. As a result, our model's performance deteriorates as our test object gets smaller. This can be visualized in the figure
        below.
      </p>
      <b>TODO: maybe give chamfer distances of the clamps + visualize clamp completed PC</b>
      <p>
        Furthermore, our model
      </p>

      <p>
        Thus, our model struggles to reconstruct geometries for objects that are small and have irregular shapes.
      </p>

      <b>TODO: significance is that we were able to apply specialize existing state of the art models into different sensor domains,
        allowing us to fine tune models for specific applications or smth
      </b>

      <b>hammer: handle and head different materials -> different reflection so point cloud detached -> can't account for this</b>
    </div>
    <div class="margin-right-block"></div>
  </div>

  <div class="content-margin-container" id="conclusion">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
      <h1>Conclusion</h1>
	  <hr>
      <p>
        In this project, we attempted to adapt state-of-the-art point cloud completion techniques to the fundamentally different sensing regime of mmWave radar. Unlike vision-based sensors,
        mmWave systems generate highly sparse, noisy, and specular measurements, making direct application of existing models ineffective. To address this challenge, we introduced a physics-aware
        synthetic data pipeline, a full-reconstruction architecture that enables implicit denoising, and a density-aware loss that promotes more uniform point distributions.
      </p>
      <p>
        Through experimentation on both synthetic data and real-world measurements from the MITO dataset, our model consistently outperforms PoinTr across Chamfer Distance and F-Score metrics.
        Qualitative results further demonstrate improved geometric fidelity, especially in cases where mmWave sparsity is severe. Latent-space analysis suggests that our architectural changes lead to
        more structured and discriminative feature representations, enabling the model to better distinguish object categories and infer missing geometry.

        Beyond performance gains, this work contributes to the broader understanding of deep learning in several ways:
        <ul>
          <li>It highlights the importance of incorporating domain-specific inductive biases into data generation and model design, rather than relying solely on generic architectures.</li>

          <li>It demonstrates how deep learning models behave under severe domain shift: models trained on vision-like data fail to generalize to mmWave inputs, whereas models equipped with the appropriate inductive biases can successfully transfer across sensing modalities.</li>

          <li>It illustrates that architectural and loss-function choices directly reshape the latent manifold: by enforcing full reconstruction and density-aware optimization, the model learns latent clusters that better reflect object categories and geometric relationships.</li>

          <li>It reinforces the broader understanding that deep learning is not just about scaling data or model size. Rather, the practice of producing meaningful datasets that capture our domain-specific properties is underscored.  </li>
        </ul>


        While our approach significantly improves reconstruction quality in comparison to PoinTr, challenges remain, especially for small or irregular objects that produce weak mmWave returns. Future work may integrate richer
        generative priors, differentiable radar simulation, or multi-sensor fusion to further enhance robustness.

        Overall, our findings demonstrate how the effectiveness of deep learning models depends critically on the inductive biases we embed into their data, architecture, and objectives. By aligning these biases with the physics of mmWave sensing,
        we move toward reliable 3D perception beyond the domain of traditional vision sensors.
      </p>
    </div>
    <div class="margin-right-block"></div>
  </div>

  <div class="content-margin-container" id="citations">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
      <div class='citation' id="references" style="height:auto"><br>
        <span style="font-size:16px">References:</span><br><br>
        <a id="ref_1"></a>[1] <a href="https://en.wikipedia.org/wiki/Allegory_of_the_cave">Allegory of the Cave</a>, Plato, c. 375 BC<br><br>
        <a id="ref_2"></a>[2] <a href="">A Human-Level AGI</a>, OpenAI, 2025<br><br>
      </div>
    </div>
    <div class="margin-right-block"></div>
  </div>

</body>
</html>
